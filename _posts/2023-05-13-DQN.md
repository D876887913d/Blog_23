---
layout: posts
title:  "DQN"
date:   2023-05-13 01:06:22 +0800
categories: jekyll update
---

## 论文内容记录

$The\ environment(or\ emulator)\ is\ The\ Arcade\ Learning\ Environment.$

进行实际仿真的时候应当采用模拟、训练相互独立的形式进行，这样的目的是提高训练的效率，减少渲染环境所消耗的电脑算力。游戏的分数是根据规定的*奖励函数*来进行计算的.
在进行实际的演算的过程中，通过设定最大的时间步长，即设定最多的运行步数(step次数)，超过后将会进入下一次迭代，并在迭代开始时进行reset().

论文中采用的是像素级别的强化学习进行的训练，通过记录整个屏幕的图像来进行训练，通过观察其中的动作数值以及状态数值的序列来进行的相关的游戏策略的设计。所有的游戏策略是一个有限的时间步长就可以实现的过程，通过MDP将每个序列作为一个确定的状态来进行马尔可夫决策过程规划的解决方法，来进行奖励的计算。每个智能体的目标均为利用模拟器来获取未来能够获取最大奖励函数的动作。我们通过定义可以选择的动作价值函数Q,通过动作价值函数来确定要采用的策略$\pi$,即采取的连续动作序列。

在动作价值函数的设置过程中，遵循的方程为贝尔曼最优方程。在迭代次数$i->\infin$的过程中，会出现Q=Q\*结果，即最终会收敛到最优动作价值函数Q\*。由于每一个序列都是相互独立的，而并非是什么不断延续下去的结果，因此我们只能通过函数拟合的方法，来构造函数$Q(s,a,\theta)\approx Q(s,a)$。

Q网络的目的是最小化序列的损失函数，即最小化损失函数的数值，类似于大多数神经网络最小化loss数值的形式，采用随机梯度下降等方式来进行正向反向传播，最终实现参数的更改，使得原本的参数数值不断接近最优的参数值，进而更好的拟合原本需要的动作价值函数Q。

### DQN算法的性质

$model\ free$

采样的方法来进行仿真器的设计，明确的构建了强化学习的估计环境。

$off-policy$

离线策略，采用贪心算法获取动作，同时允许一定的状态空间的探索，具体采用的是$\epsilon-greedy$算法来进行这种目的的实现。

## 相关研究内容记录

> $TD-gammon$

时序差分作为一种无需模型的RL算法，类似于$Q-learning$算法，估计价值函数为一层隐含层的多层感知机。由于在跳棋上并没有产生很好的效果，因此被认为是一种仅适用于十五子棋的一种算法，由于其掷骰子的随机性，导致价值函数特别平滑。

结合$Q-learning$算法以及非线性估计函数离线学习策略将会导致出现不收敛(发散)的情况出现。大多数的RL研究都采用线性估计函数以保证函数能够更好的收敛。

深度神经网络与RL结合： \* DNN已经用来去估计环境$\epsilon$. \*
限制玻尔兹曼机器用于去估计价值函数。 \*
Q-learning算法的不收敛问题利用梯度时序差分方法来解决。这种方法已经被证明可以在非线性估计函数中收敛，当采用线性估计函数学习控制律的过程中采用Q-learning,
这些方法也被扩展到用于非线性控制律。

$Neural\ Fitted\ Q-learning$\
采用RPROP算法更新Q网络的参数，采用批量更新的方式每次根据批量占总数据个数的比例进行损失函数的更新。将输入的视觉信息与RL网络进行端对端的连接，采用视觉层提取低维信息，并通过NFQ算法来进行表示。此后也有HyperNEAT
Evolutionary architecture来作为架构引入到训练过程。

### 文章内容

$Deep\ Reinforcement\ Learning$\
当前最为有效的训练方法为直接读取`raw inputs`，然后采用随机梯度下降算法来进行轻量化更新。输入足够多的特征，比手工制作一些特征的效果可能会更好，因为数据的个数的到了许多个个数量级上的提升。因此直接读入RGB图像，并采用梯度下降算法进行参数更新一般能够取得较好的训练的效果。

TD-Gammon
架构对于这样的一种方式产生了一个起始值，这种架构根据估计价值函数来进行参数更新，并采用了在线策略进行训练，利用当前状态、当前动作、奖励值、下一个状态、下一个动作来进行与环境的交互。随着二十年的硬件更新以及DNN架构的更新以及大小可变的RL算法也许能够产生更有意义的更新。

与TD-Gammon不同的是，本篇文章采用了经验回放的技术用以记录每次的智能体经验($e_t=( s_t, a_t, r_t, s_{t+1} )$),储备许多经验在经验记忆区，最终经验记忆区为一个数据集合$Data-set\ D=(e_1,...e_N)$。采用Q-learning算法以及
MINI-batch 算法更新，进行经验回放之后之后采用 $\epsilon-greedy$
算法来进行 action 的选取。这个算法就被叫做 DQN 算法。

![$DQN Algorithm$]({{ site.url }}/img/算法1.png)

DQN 算法的优于在线的 Q-learning 算法主要体现在： 
* 每一步的经验都会用于多次权重的更新，处理大的数据更加高效； 
* 采用经验回放的数据能够防止出现相关性较强的数据，使得数据满足独立性。 
* 通过采用经验回放可以让动作选取采用当前状态与多个此前的状态平滑后的结果作为最后的动作选取方式，能够防止函数发散以及出现震荡。

在算法设定的过程中，通过固定经验回放区的经验集合个数为N，从集合D中进行均匀采样以进行权重的更新。这种方法是有一定的限制的因为内存缓冲区是有限的，因此不能区分重要的转变，而且总会覆盖原本的变换。类似的，均匀采样采样时对于每个样本是平等的，并不会根据样本的重要程度来进行选择。一个更好的采样策略应当是按照重要性进行采样的，例如$Prioritized\ Sweeping$ 方法。

### Method

直接在 $raw\ Atari\ Frames$ 进行作用,每一帧图像都为210 $\times$ 160像素、128灰度值的图像，将图像转为灰度图，并将图像降采样，使图像的尺寸变为110 $\times$ 84。由于 GPU 需要的输入格式为正方形的格式，因此采用 crop  操作将原本的图像转为了 84 $\times$ 84 的图像作为最终的输入。
采用神经网络来对 Q 值进行估计，由于 Q 映射的是过去的动作的组合，用来去估计 Q 值、过去采用的动作、过去的一些状态动作元组等，这种结构主要的缺点是前向传播的时候需要对所有的动作计算Q值，导致了线性的损失数值。可以替换为将输出单元根据不同的动作进行分离，仅有状态代表作为输入送入神经网络。输出对应的是单独动作的 Q 值，对于特定的状态。主要的优势就是能够对于所有的动作计算 Q 值，在神经网络训练的过程中就只需要进行前向传播即可。

网络结构上，第一个网络结构的隐含层由 16 个 8 $\times$ 8 的卷积核组成，步长为4，采用了一个非线性整流器。第二个隐含层由 32 个 4 $\times$ 4 的卷积核组成，步长为2，同样采用了非线性整流器。最后一个隐含层为全连接器由 256 个整流器单元组成。 输出层为全连接层，输出只有一个，范围为 4 ~ 18，对应到游戏里的指定动作。
 
### Experiment 
采用了七个流行的 $ATARI$ 的游戏，分别为 Beam Rider 、 Breakout 、 Enduro 、 Pong 、 Q* bert 、 Seaquest 、 Space Invader. 使用了相同的网络结构、学习算法、以及超参数设置，以便于验证模型的鲁棒性，同时只截取游戏的画面，并没有采取相应的游戏数据结构来进行专门的设计。为了归一化其中的奖励值以及惩罚值，将其中的奖励值设为 1 ，惩罚值设为 -1 ，不变的话设置为 0 。这样的话能够防止在各个游戏里面的奖励值不同，导致对应的奖励函数收敛到不精确的位置。  
采用了 $RMSProp( Root Mean Square Propagation )$ 算法来进行自适应学习率的梯度下降。采用了指数加权移动平均的算法来进行迭代过程中 loss 函数的平滑性，并将 $mini - batch$ 的个数设置为 32 。采用 $\epsilon-greedy$ 算法进行动作的选取，并采用的线性退火算法将学习率在 100,0000 个像素帧从 1 变化到0.1. 训练了 10 个 100,0000 帧像素帧，采用记忆回放将最常用的 100,0000 个像素帧来进行记录。采用了跳帧的技术进行训练，分别采用了每 k(1,2,3,4) 个帧数来提取一帧，以令激光变成一道光束，而不是光点。

### Train
在训练的过程中，分别采用了 record 、 Q 值的平均值作为评估指标，作为测试指标而言，Q 值最终的评估结果变化较为稳定，而 record 作为评估指标的时候，最终的数据震荡数值较大，对于不同的输入、动作的鲁棒性较差。


## Formula Record

奖励函数： $R_t=\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$

动作价值函数： $Q*(s,a)=max_{\pi} E[R_t\|s_t=s,a_t=a,\pi]$

贝尔曼最优方程：
$Q*(s,a)=E_{s'-\epsilon}[r+\gamma max_{a'}Q*(s',a')\|s,a]$

损失函数： $L_i(\theta_i)=E_{s.a-\rho(·)}(y_i-Q(s,a;\theta_i))^2$
